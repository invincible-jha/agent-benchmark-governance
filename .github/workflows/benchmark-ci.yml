# SPDX-License-Identifier: Apache-2.0
# Copyright (c) 2026 MuVeraAI Corporation
#
# benchmark-ci.yml
# Runs the agent governance benchmark suite on demand and on a weekly schedule.
# Results are uploaded as workflow artifacts and the leaderboard data file
# is updated for any implementations that ship a bundled adapter.

name: Governance Benchmark CI

on:
  # Allow manual triggers with suite selection
  workflow_dispatch:
    inputs:
      suite:
        description: "Scenario suite to run"
        required: true
        default: "basic"
        type: choice
        options:
          - basic
          - full
      adapter:
        description: "Adapter module to benchmark (dotted Python path)"
        required: false
        default: "governance_benchmark.baselines.PassthroughAdapter"

  # Weekly run every Monday at 06:00 UTC â€” covers both suites
  schedule:
    - cron: "0 6 * * 1"

permissions:
  contents: read

jobs:
  benchmark:
    name: "Benchmark (${{ matrix.suite }})"
    runs-on: ubuntu-latest

    strategy:
      fail-fast: false
      matrix:
        suite:
          - basic
          - full

        # On scheduled runs, always run both suites.
        # On manual dispatch, run only the requested suite.
        exclude:
          - suite: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.suite != 'basic' && 'basic' || '' }}
          - suite: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.suite != 'full' && 'full' || '' }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Set up Python 3.12
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
          cache: pip

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"

      - name: Determine scenario categories for suite
        id: suite-config
        run: |
          if [ "${{ matrix.suite }}" = "basic" ]; then
            echo "categories=trust_escalation,budget_abuse,consent_violation,identity_spoofing" >> "$GITHUB_OUTPUT"
          else
            echo "categories=trust_escalation,budget_abuse,memory_leak,consent_violation,identity_spoofing,cross_domain_leakage,social_engineering,privilege_escalation,red_team" >> "$GITHUB_OUTPUT"
          fi

      - name: Run benchmark via pytest
        env:
          BENCHMARK_SUITE: ${{ matrix.suite }}
          BENCHMARK_CATEGORIES: ${{ steps.suite-config.outputs.categories }}
          BENCHMARK_ADAPTER: ${{ github.event.inputs.adapter || 'governance_benchmark.baselines.PassthroughAdapter' }}
        run: |
          pytest tests/ \
            -v \
            --tb=short \
            --junit-xml=results/junit-${{ matrix.suite }}.xml \
            -m "benchmark" \
            || true

      - name: Export benchmark results to JSON
        run: |
          mkdir -p results
          python -m governance_benchmark.cli \
            --adapter "${{ github.event.inputs.adapter || 'governance_benchmark.baselines.PassthroughAdapter' }}" \
            --categories "${{ steps.suite-config.outputs.categories }}" \
            --output "results/benchmark-${{ matrix.suite }}-$(date -u +%Y%m%dT%H%M%SZ).json" \
            --format json \
            || echo '{"error": "cli_not_available"}' > "results/benchmark-${{ matrix.suite }}-fallback.json"

      - name: Upload benchmark results artifact
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ matrix.suite }}-${{ github.run_id }}
          path: results/
          retention-days: 90

  update-leaderboard:
    name: "Update Leaderboard Data"
    needs: benchmark
    runs-on: ubuntu-latest
    # Only update the leaderboard on scheduled runs to avoid noise from manual dispatches.
    if: github.event_name == 'schedule'
    permissions:
      contents: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Download full-suite results
        uses: actions/download-artifact@v4
        with:
          name: benchmark-results-full-${{ github.run_id }}
          path: results/

      - name: Set up Python 3.12
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
          cache: pip

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"

      - name: Refresh leaderboard data.json
        run: |
          python scripts/update_leaderboard.py \
            --results-dir results/ \
            --leaderboard leaderboard/data.json \
            || echo "Leaderboard update skipped (script not yet available)"

      - name: Commit updated leaderboard
        run: |
          git config user.name  "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add leaderboard/data.json
          git diff --cached --quiet || git commit -m "chore(benchmark): update leaderboard from scheduled CI run ${{ github.run_id }}"
          git push
